{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 1) \n",
      " [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.  1.]\n",
      " [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.\n",
      "   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.\n",
      "  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.\n",
      "   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.  2.]\n",
      " [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.\n",
      "   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.\n",
      "  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.\n",
      "   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.  3.]\n",
      " [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.\n",
      "   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.\n",
      "   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.\n",
      "   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.  4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.datasets import load_digits\n",
    "iris = load_digits(); X = iris.data.astype(np.float16);\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1);\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (360, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron implementation:** $\\;$ returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ also the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):\n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break;\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:** $\\;$ Perceptron minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  106\n",
      "Number of training errors:  0\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[-1.420e+02 -2.120e+02 -1.420e+02 -1.430e+02 -1.040e+02 -1.480e+02\n",
      "  -1.390e+02 -1.410e+02 -1.450e+02 -1.910e+02]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
      " [-1.700e+01 -2.600e+01  3.300e+01  5.000e+01 -3.500e+01 -1.300e+01\n",
      "  -4.200e+01  4.200e+01 -5.700e+01 -1.720e+02]\n",
      " [-8.910e+02 -1.029e+03 -9.100e+02 -9.130e+02 -8.400e+02 -5.020e+02\n",
      "  -1.030e+03 -8.650e+02 -8.160e+02 -9.250e+02]\n",
      " [-1.639e+03 -1.522e+03 -1.635e+03 -1.680e+03 -2.119e+03 -1.819e+03\n",
      "  -1.746e+03 -1.539e+03 -1.805e+03 -1.457e+03]\n",
      " [-1.708e+03 -2.382e+03 -1.691e+03 -1.321e+03 -1.808e+03 -1.673e+03\n",
      "  -1.757e+03 -1.597e+03 -1.682e+03 -1.599e+03]\n",
      " [-1.084e+03 -4.850e+02 -9.800e+02 -9.290e+02 -1.280e+03 -6.760e+02\n",
      "  -1.110e+03 -8.550e+02 -1.067e+03 -1.035e+03]\n",
      " [-3.240e+02 -2.790e+02 -2.640e+02 -2.130e+02 -3.290e+02  1.420e+02\n",
      "  -1.300e+02  0.000e+00 -4.330e+02 -4.500e+01]\n",
      " [-3.200e+01 -2.000e+01 -1.400e+01 -3.500e+01  9.000e+01 -4.600e+01\n",
      "  -1.500e+01  2.700e+01 -5.700e+01 -6.500e+01]\n",
      " [ 0.000e+00 -2.000e+00 -1.400e+01 -1.000e+01  0.000e+00  4.000e+00\n",
      "   0.000e+00  0.000e+00  1.600e+01  0.000e+00]\n",
      " [-3.260e+02 -5.750e+02 -7.400e+01 -2.160e+02 -2.350e+02 -3.760e+02\n",
      "  -4.530e+02 -1.920e+02 -2.570e+02 -2.350e+02]\n",
      " [-1.814e+03 -2.179e+03 -1.755e+03 -1.546e+03 -1.944e+03 -1.621e+03\n",
      "  -1.901e+03 -1.557e+03 -1.720e+03 -1.684e+03]\n",
      " [-1.701e+03 -2.024e+03 -2.052e+03 -1.740e+03 -2.012e+03 -1.862e+03\n",
      "  -1.815e+03 -1.690e+03 -1.749e+03 -1.859e+03]\n",
      " [-1.730e+03 -1.614e+03 -1.514e+03 -1.547e+03 -2.071e+03 -1.739e+03\n",
      "  -2.005e+03 -1.333e+03 -1.995e+03 -1.829e+03]\n",
      " [-1.363e+03 -1.564e+03 -1.445e+03 -1.341e+03 -1.508e+03 -1.467e+03\n",
      "  -1.377e+03 -1.475e+03 -1.363e+03 -1.452e+03]\n",
      " [-3.790e+02 -5.790e+02 -3.240e+02 -1.250e+02 -4.110e+02 -3.120e+02\n",
      "  -2.630e+02 -3.770e+02 -1.710e+02 -2.400e+02]\n",
      " [-3.000e+01 -1.700e+01 -1.300e+01 -3.300e+01  7.600e+01 -2.600e+01\n",
      "  -2.000e+00  3.400e+01 -1.400e+01 -6.700e+01]\n",
      " [ 0.000e+00  0.000e+00 -6.000e+00 -6.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  9.000e+00  0.000e+00]\n",
      " [-3.920e+02 -2.750e+02 -5.250e+02 -4.570e+02 -3.780e+02 -5.120e+02\n",
      "  -5.500e+02 -7.320e+02 -3.050e+02 -3.940e+02]\n",
      " [-1.667e+03 -1.791e+03 -1.808e+03 -2.076e+03 -1.614e+03 -1.654e+03\n",
      "  -1.597e+03 -1.980e+03 -1.629e+03 -1.711e+03]\n",
      " [-1.472e+03 -1.055e+03 -1.710e+03 -1.821e+03 -1.356e+03 -1.429e+03\n",
      "  -1.419e+03 -1.669e+03 -1.489e+03 -1.425e+03]\n",
      " [-1.767e+03 -1.204e+03 -1.275e+03 -1.515e+03 -1.553e+03 -1.961e+03\n",
      "  -1.762e+03 -1.491e+03 -1.585e+03 -1.393e+03]\n",
      " [-1.162e+03 -1.474e+03 -1.378e+03 -1.573e+03 -1.208e+03 -1.616e+03\n",
      "  -1.603e+03 -1.270e+03 -1.238e+03 -8.920e+02]\n",
      " [-2.050e+02 -8.400e+01  4.800e+01 -2.130e+02 -2.150e+02 -7.680e+02\n",
      "  -3.040e+02 -1.460e+02 -2.290e+02 -2.260e+02]\n",
      " [-3.000e+00 -7.000e+00  0.000e+00 -1.000e+01  5.400e+01 -1.100e+01\n",
      "  -2.000e+00 -3.000e+00 -3.000e+00 -3.200e+01]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
      " [-2.000e+02 -4.870e+02 -4.870e+02 -5.400e+02 -3.550e+02 -2.060e+02\n",
      "  -8.400e+01 -4.540e+02 -2.400e+02 -3.720e+02]\n",
      " [-1.400e+03 -1.460e+03 -1.863e+03 -1.782e+03 -1.172e+03 -1.222e+03\n",
      "  -1.377e+03 -1.440e+03 -1.610e+03 -1.293e+03]\n",
      " [-1.791e+03 -1.758e+03 -2.103e+03 -1.986e+03 -1.605e+03 -1.743e+03\n",
      "  -1.661e+03 -1.992e+03 -1.529e+03 -1.485e+03]\n",
      " [-1.964e+03 -1.494e+03 -1.858e+03 -1.469e+03 -1.568e+03 -1.481e+03\n",
      "  -1.791e+03 -1.564e+03 -1.698e+03 -1.727e+03]\n",
      " [-1.125e+03 -8.970e+02 -1.156e+03 -1.372e+03 -1.276e+03 -1.184e+03\n",
      "  -1.096e+03 -1.066e+03 -1.065e+03 -8.930e+02]\n",
      " [-1.400e+02 -2.640e+02 -1.390e+02 -4.890e+02  2.500e+01 -4.740e+02\n",
      "  -3.720e+02  5.400e+01 -1.790e+02 -1.470e+02]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  6.000e+00 -1.000e+00\n",
      "  -1.000e+00 -3.000e+00 -1.000e+00 -3.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
      " [-4.200e+01 -1.970e+02 -2.840e+02 -1.820e+02  7.000e+00 -9.100e+01\n",
      "   5.500e+01 -1.060e+02 -3.380e+02 -5.430e+02]\n",
      " [-8.840e+02 -7.500e+02 -1.178e+03 -1.057e+03 -7.980e+02 -7.990e+02\n",
      "  -9.230e+02 -8.320e+02 -1.042e+03 -9.690e+02]\n",
      " [-1.642e+03 -1.590e+03 -1.592e+03 -1.441e+03 -1.543e+03 -1.683e+03\n",
      "  -1.302e+03 -1.553e+03 -1.180e+03 -1.327e+03]\n",
      " [-1.802e+03 -1.560e+03 -1.555e+03 -1.479e+03 -1.401e+03 -1.842e+03\n",
      "  -1.591e+03 -1.465e+03 -1.562e+03 -1.684e+03]\n",
      " [-1.242e+03 -1.110e+03 -1.365e+03 -1.396e+03 -9.910e+02 -1.214e+03\n",
      "  -1.348e+03 -1.032e+03 -1.459e+03 -1.246e+03]\n",
      " [-2.370e+02 -4.490e+02 -4.360e+02 -1.000e+02  1.240e+02 -1.270e+02\n",
      "   3.000e+00 -5.800e+01 -6.250e+02 -4.180e+02]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
      " [-2.010e+02 -3.450e+02 -1.500e+01 -1.950e+02  3.400e+02 -2.400e+02\n",
      "  -3.420e+02 -1.290e+02 -1.750e+02 -1.970e+02]\n",
      " [-7.450e+02 -1.026e+03 -9.230e+02 -1.136e+03 -8.790e+02 -1.248e+03\n",
      "  -5.680e+02 -9.620e+02 -6.100e+02 -1.321e+03]\n",
      " [-1.026e+03 -7.440e+02 -7.410e+02 -1.502e+03 -6.430e+02 -1.178e+03\n",
      "  -9.530e+02 -1.061e+03 -1.059e+03 -1.685e+03]\n",
      " [-1.292e+03 -1.319e+03 -1.665e+03 -1.087e+03 -9.670e+02 -1.324e+03\n",
      "  -1.066e+03 -1.206e+03 -9.740e+02 -1.639e+03]\n",
      " [-1.173e+03 -1.388e+03 -1.532e+03 -1.057e+03 -1.105e+03 -1.363e+03\n",
      "  -1.218e+03 -1.414e+03 -1.148e+03 -1.592e+03]\n",
      " [-3.760e+02 -7.310e+02 -4.770e+02 -7.500e+01 -1.330e+02 -3.270e+02\n",
      "  -2.220e+02 -3.360e+02 -1.860e+02 -7.040e+02]\n",
      " [ 0.000e+00 -2.000e+00  4.000e+00 -4.000e+00  0.000e+00  0.000e+00\n",
      "   4.000e+00  0.000e+00 -2.000e+00  0.000e+00]\n",
      " [ 0.000e+00 -3.000e+00  3.000e+00  0.000e+00 -3.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00 -3.000e+00  0.000e+00]\n",
      " [-2.120e+02 -4.700e+01  4.100e+01 -1.680e+02  8.000e+01  3.700e+01\n",
      "  -2.080e+02 -1.560e+02 -1.590e+02 -4.700e+01]\n",
      " [-1.014e+03 -1.273e+03 -1.151e+03 -1.221e+03 -1.440e+03 -1.263e+03\n",
      "  -1.040e+03 -1.252e+03 -9.960e+02 -1.314e+03]\n",
      " [-1.400e+03 -1.303e+03 -9.770e+02 -1.624e+03 -1.398e+03 -1.603e+03\n",
      "  -1.399e+03 -1.363e+03 -1.634e+03 -1.575e+03]\n",
      " [-1.388e+03 -1.096e+03 -1.073e+03 -1.412e+03 -1.407e+03 -1.350e+03\n",
      "  -1.518e+03 -1.549e+03 -1.708e+03 -1.407e+03]\n",
      " [-1.303e+03 -1.368e+03 -1.169e+03 -1.215e+03 -1.674e+03 -1.314e+03\n",
      "  -1.208e+03 -1.736e+03 -1.322e+03 -1.348e+03]\n",
      " [-4.420e+02 -6.290e+02 -2.890e+02 -9.800e+01 -5.860e+02 -6.660e+02\n",
      "  -3.170e+02 -4.880e+02 -2.470e+02 -3.440e+02]\n",
      " [-2.200e+01  9.700e+01  6.000e+00 -1.120e+02 -9.000e+00 -1.500e+01\n",
      "  -8.900e+01 -4.000e+00 -3.000e+01  6.100e+01]\n",
      " [ 0.000e+00 -1.000e+00  1.000e+00  0.000e+00 -1.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00 -1.000e+00  0.000e+00]\n",
      " [-1.700e+01 -1.000e+00  1.040e+02 -2.100e+01 -8.100e+01  3.700e+01\n",
      "  -4.000e+01 -1.000e+01 -5.400e+01 -1.120e+02]\n",
      " [-8.820e+02 -1.102e+03 -7.280e+02 -6.160e+02 -8.080e+02 -5.290e+02\n",
      "  -9.310e+02 -9.030e+02 -1.224e+03 -8.450e+02]\n",
      " [-1.500e+03 -1.599e+03 -1.605e+03 -1.583e+03 -1.861e+03 -1.450e+03\n",
      "  -1.812e+03 -1.864e+03 -1.716e+03 -1.565e+03]\n",
      " [-1.746e+03 -1.750e+03 -1.612e+03 -1.754e+03 -1.723e+03 -1.673e+03\n",
      "  -1.716e+03 -1.793e+03 -1.454e+03 -1.672e+03]\n",
      " [-1.090e+03 -1.048e+03 -9.650e+02 -1.082e+03 -1.321e+03 -9.950e+02\n",
      "  -1.021e+03 -1.305e+03 -1.267e+03 -1.053e+03]\n",
      " [-1.950e+02 -4.300e+01  8.200e+01 -1.580e+02 -3.160e+02 -4.380e+02\n",
      "  -2.200e+02 -3.670e+02 -4.180e+02 -1.730e+02]\n",
      " [-4.200e+01  1.600e+02  1.090e+02 -1.230e+02 -9.000e+00 -6.000e+01\n",
      "  -1.320e+02 -1.000e+01 -1.440e+02 -3.500e+01]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = perceptron(X_train, y_train)\n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 4.4%\n"
     ]
    }
   ],
   "source": [
    "X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Margin adjustment:** $\\;$ experiment to learn a value of $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 106\n",
      "0.01 0 106\n",
      "0.1 0 106\n",
      "10 0 140\n",
      "100 0 97\n"
     ]
    }
   ],
   "source": [
    "for b in (.0, .01, .1, 10, 100):\n",
    "    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n",
    "    print(b, E, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

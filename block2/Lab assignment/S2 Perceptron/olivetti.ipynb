{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096) (400, 1) \n",
      " [[0.30981445 0.36767578 0.41723633 ... 0.16113281 0.15698242 0.        ]\n",
      " [0.45458984 0.47119141 0.51220703 ... 0.15283203 0.15283203 0.        ]\n",
      " [0.31811523 0.40087891 0.49169922 ... 0.14880371 0.15283203 0.        ]\n",
      " [0.19836426 0.19421387 0.19421387 ... 0.75195312 0.73974609 0.        ]\n",
      " [0.5        0.54541016 0.58251953 ... 0.17358398 0.17358398 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.datasets import fetch_olivetti_faces\n",
    "iris = fetch_olivetti_faces(); X = iris.data.astype(np.float16);\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1);\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 4096) (80, 4096)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron implementation:** $\\;$ returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ also the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):\n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break;\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:** $\\;$ Perceptron minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  42\n",
      "Number of training errors:  0\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[-587.         -581.         -581.         ... -590.\n",
      "  -557.         -585.        ]\n",
      " [-247.08929443 -233.21960449 -244.16125488 ... -266.64141846\n",
      "  -244.67816162 -239.23931885]\n",
      " [-265.13061523 -251.18511963 -267.29406738 ... -281.07702637\n",
      "  -270.37103271 -256.94555664]\n",
      " ...\n",
      " [-211.28652954 -203.89273071 -199.72198486 ... -196.6746521\n",
      "  -201.48944092 -207.01318359]\n",
      " [-205.98419189 -202.98080444 -198.18414307 ... -194.52960205\n",
      "  -195.22445679 -199.60604858]\n",
      " [-206.72164917 -203.47958374 -198.56820679 ... -189.42913818\n",
      "  -192.849823   -194.80505371]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = perceptron(X_train, y_train)\n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 7.5%\n"
     ]
    }
   ],
   "source": [
    "X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Margin adjustment:** $\\;$ experiment to learn a value of $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 48\n",
      "0.01 0 58\n",
      "0.1 0 42\n",
      "10 0 45\n",
      "100 0 72\n"
     ]
    }
   ],
   "source": [
    "for b in (.0, .01, .1, 10, 100):\n",
    "    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n",
    "    print(b, E, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

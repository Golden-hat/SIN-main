{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exam of block (Group 3E1 Turn 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exam, we will be using one of the classification tasks found in OpenML. More precisely, a 10% stratified subsample of the task [*KDDCup99*](https://www.openml.org/search?type=data&id=1113) (data_id=1113) is selected. The classification goal of this task is to predict whether a connection is normal or an attack, with exactly one specific attack type. The input features are basic, content and traffic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find a baseline result achieved with the logistic regression classifier using default parameters devoting 9% to training and 1% to test (random_state=23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:   1.8%\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\"); import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_id = 1113\n",
    "train_size = 0.09\n",
    "test_size = 0.01\n",
    "X, y = fetch_openml(data_id=data_id, return_X_y=True, as_frame=False)\n",
    "# Default parameter values: tol=1e-4, C=1e0, solver='lbfgs', max_iter=1e2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = train_size, test_size=test_size, random_state=23)\n",
    "clf = LogisticRegression(random_state=23).fit(X_train, y_train)\n",
    "print(f'Test error: {(1 - accuracy_score(y_test, clf.predict(X_test)))*100:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Applying the logistic regression classifier with default parameter values except for the solver, explore different solvers to find that optimal. Report classification error rate on training and test sets. Use random_state=23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error after training with the solver lbfgs: 1.8%\n",
      "Test error after training with the solver liblinear: 0.6%\n",
      "Test error after training with the solver newton-cg: 0.5%\n",
      "Test error after training with the solver newton-cholesky: 0.1%\n",
      "Test error after training with the solver sag: 29.6%\n",
      "Test error after training with the solver saga: 29.6%\n"
     ]
    }
   ],
   "source": [
    "for solver in ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']:\n",
    "    clf = LogisticRegression(random_state=23, solver=solver, max_iter=100).fit(X_train, y_train)\n",
    "    err_test = 1 - accuracy_score(y_test, clf.predict(X_test))\n",
    "    print(f\"Test error after training with the solver {solver!s}: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best solver for the regression classifier for this data set is the newton-cholesky one, because it provides the least amount of errors out of all. This means that it is capable of classifying 99.9% of the data in their correct and corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Applying the logistic regression classifier with default parameter values except for the parameter C and the best solver from exercise 1, explore the values of the parameter C in logarithmic scale to determine an optimal value. Report classification error rate on training and test sets. Use random_state=23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error with C 0.01: 0.465%\n",
      "Test error with C 0.1: 0.283%\n",
      "Test error with C 1: 0.121%\n",
      "Test error with C 10: 0.081%\n",
      "Test error with C 100: 0.061%\n"
     ]
    }
   ],
   "source": [
    "for C in (1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    clf = LogisticRegression(C=C, random_state=23, solver='newton-cholesky', max_iter=10000).fit(X_train, y_train)\n",
    "    err_test = 1 - accuracy_score(y_test, clf.predict(X_test))\n",
    "    print(f\"Test error with C {C:g}: {err_test:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of C is the one performed with C = 100, as it provides the value closest to zero, and thus the maximum regularization for the adjustment performed by the logistic regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Applying the logistic regression classifier with default parameter values except for the maximum number of iterations, the best solver and the optimal value for the C value from previous exercises, explore the maximum number of iterations in logarithmic scale to determine an optimal value. Report classification error rate on training and test sets. Use random_state=23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error with max_iter 2: 0.3238210888%\n",
      "Test error with max_iter 4: 0.1821493625%\n",
      "Test error with max_iter 8: 0.1214329083%\n",
      "Test error with max_iter 16: 0.0607164542%\n",
      "Test error with max_iter 32: 0.0607164542%\n"
     ]
    }
   ],
   "source": [
    "for max_iter in (2, 4, 8, 16, 32):\n",
    "    clf = LogisticRegression(C=100, solver='newton-cholesky', random_state=23, max_iter=max_iter).fit(X_train, y_train)\n",
    "    err_test = 1 - accuracy_score(y_test, clf.predict(X_test))\n",
    "    print(f\"Test error with max_iter {max_iter}: {err_test:.10%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using powers of two as our baseline for testing (log scale of base 2, as they provide us with a more meaningful result), we can see that the optimal amount of iterations is provided in the range around 8 and 16 given our C value and our solver. A number of iterations under this range will not provide a great estimation (with significant error), but a number of iterations over it won't either enhance the accuracy of the regression algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
